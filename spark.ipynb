{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939bf98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.1-spark_3.1/spark-snowflake_2.12-2.9.1-spark_3.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.1-spark_3.1/spark-snowflake_2.12-2.9.1-spark_3.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6.pom\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6.pom\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.1-spark_3.1/spark-snowflake_2.12-2.9.1-spark_3.1-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3.jar\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.1-spark_3.1/spark-snowflake_2.12-2.9.1-spark_3.1.jar\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6.jar\n",
      "Downloading https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.1-spark_3.1/spark-snowflake_2.12-2.9.1-spark_3.1-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.1-spark_3.1/spark-snowflake_2.12-2.9.1-spark_3.1.jar\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/snowflake-ingest-sdk/0.10.3/snowflake-ingest-sdk-0.10.3.jar\n",
      "Still downloading:\n",
      "https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6.jar (83.24 %, 23560192 / 28305411)\n",
      "\n",
      "Downloaded https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.6/snowflake-jdbc-3.13.6.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import the Spark library and the Snowflake Spark Connector from Maven.\n",
    "import $ivy.`org.apache.spark:spark-core_2.12:3.1.2`\n",
    "import $ivy.`org.apache.spark:spark-sql_2.12:3.1.2`\n",
    "import $ivy.`net.snowflake:snowflake-jdbc:3.13.8`\n",
    "\n",
    "import $ivy.`net.snowflake:spark-snowflake_2.12:2.9.1-spark_3.1`\n",
    "import $ivy.`net.snowflake:snowflake-ingest-sdk:0.10.3`\n",
    "\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14a6489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 05:41:49 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/11/07 05:41:49 INFO ResourceUtils: ==============================================================\n",
      "21/11/07 05:41:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/11/07 05:41:49 INFO ResourceUtils: ==============================================================\n",
      "21/11/07 05:41:49 INFO SparkContext: Submitted application: Hello Spark App\n",
      "21/11/07 05:41:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/11/07 05:41:49 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/11/07 05:41:49 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/11/07 05:41:49 INFO SecurityManager: Changing view acls to: jovyan\n",
      "21/11/07 05:41:49 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "21/11/07 05:41:49 INFO SecurityManager: Changing view acls groups to: \n",
      "21/11/07 05:41:49 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/11/07 05:41:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "21/11/07 05:41:49 INFO Utils: Successfully started service 'sparkDriver' on port 46357.\n",
      "21/11/07 05:41:49 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/11/07 05:41:49 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/11/07 05:41:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/11/07 05:41:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/11/07 05:41:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/11/07 05:41:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9451e4e3-183d-41a0-8b53-ad3c608e5967\n",
      "21/11/07 05:41:49 INFO MemoryStore: MemoryStore started with capacity 561.6 MiB\n",
      "21/11/07 05:41:49 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/11/07 05:41:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/11/07 05:41:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://6f8a053c7161:4040\n",
      "21/11/07 05:41:50 INFO Executor: Starting executor ID driver on host 6f8a053c7161\n",
      "21/11/07 05:41:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45659.\n",
      "21/11/07 05:41:50 INFO NettyBlockTransferService: Server created on 6f8a053c7161:45659\n",
      "21/11/07 05:41:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/11/07 05:41:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6f8a053c7161, 45659, None)\n",
      "21/11/07 05:41:50 INFO BlockManagerMasterEndpoint: Registering block manager 6f8a053c7161:45659 with 561.6 MiB RAM, BlockManagerId(driver, 6f8a053c7161, 45659, None)\n",
      "21/11/07 05:41:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6f8a053c7161, 45659, None)\n",
      "21/11/07 05:41:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6f8a053c7161, 45659, None)\n",
      "21/11/07 05:41:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse').\n",
      "21/11/07 05:41:50 INFO SharedState: Warehouse path is 'file:/home/jovyan/work/spark-warehouse'.\n",
      "21/11/07 05:41:50 WARN DefaultJDBCWrapper$: JDBC 3.13.8 is being used. But the certified JDBC version 3.13.6 is recommended.\n",
      "21/11/07 05:41:50 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: select * from (SELECT 1 as A, 2 as B) where 1 = 0 \n",
      "21/11/07 05:41:51 WARN DefaultJDBCWrapper$: JDBC 3.13.8 is being used. But the certified JDBC version 3.13.6 is recommended.\n",
      "21/11/07 05:41:51 WARN DefaultJDBCWrapper$: JDBC 3.13.8 is being used. But the certified JDBC version 3.13.6 is recommended.\n",
      "21/11/07 05:41:52 INFO SnowflakeSQLStatement: Spark Connector Master: execute query without bind variable: alter session set timezone = 'Etc/UTC', \n",
      "timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3',\n",
      "timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3',\n",
      "timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3';\n",
      "        \n",
      "21/11/07 05:41:52 INFO SnowflakeSQLStatement: Spark Connector Master: execute query without bind variable: SELECT * FROM ( SELECT ( CAST ( \"SUBQUERY_0\".\"A\" AS VARCHAR ) ) AS \"SUBQUERY_1_COL_0\" , ( CAST ( \"SUBQUERY_0\".\"B\" AS VARCHAR ) ) AS \"SUBQUERY_1_COL_1\" FROM ( SELECT * FROM ( ( SELECT 1 as A, 2 as B ) ) AS \"SF_CONNECTOR_QUERY_ALIAS\" ) AS \"SUBQUERY_0\" ) AS \"SUBQUERY_1\" LIMIT 21 \n",
      "21/11/07 05:41:52 INFO SnowflakeRelation: Spark Connector Master: Total statistics: partitionCount=1 rowCount=1 compressSize=1.14 KB unCompressSize=1.14 KB QueryTime=230 ms QueryID=01a01f95-0402-f186-0000-1c5500cff182\n",
      "21/11/07 05:41:52 INFO SnowflakeRelation: Spark Connector Master: Average statistics per partition: rowCount=1 compressSize=1.14 KB unCompressSize=1.14 KB\n",
      "21/11/07 05:41:53 INFO SparkContext: Starting job: show at cmd5.sc:24\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Got job 0 (show at cmd5.sc:24) with 1 output partitions\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Final stage: ResultStage 0 (show at cmd5.sc:24)\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Missing parents: List()\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at show at cmd5.sc:24), which has no missing parents\n",
      "21/11/07 05:41:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.3 KiB, free 561.6 MiB)\n",
      "21/11/07 05:41:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 561.6 MiB)\n",
      "21/11/07 05:41:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6f8a053c7161:45659 (size: 6.0 KiB, free: 561.6 MiB)\n",
      "21/11/07 05:41:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at cmd5.sc:24) (first 15 tasks are for partitions Vector(0))\n",
      "21/11/07 05:41:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/11/07 05:41:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (6f8a053c7161, executor driver, partition 0, PROCESS_LOCAL, 10059 bytes) taskResourceAssignments Map()\n",
      "21/11/07 05:41:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/11/07 05:41:53 INFO SnowflakeResultSetRDD$: Spark Connector Worker: Start reading partition ID:0 expectedRowCount= 1 TaskInfo: {  \"task_partition_id\" : 0,  \"task_attempt_id\" : 0,  \"task_attempt_number\" : 0,  \"task_stage_attempt_number\" : 0,  \"task_stage_id\" : 0,  \"thread_id\" : 162}\n",
      "21/11/07 05:41:53 INFO SnowflakeResultSetRDD$: Spark Connector Worker: Finish reading partition ID:0 expectedRowCount=1 actualReadRowCount=1\n",
      "21/11/07 05:41:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1081 bytes result sent to driver\n",
      "21/11/07 05:41:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 76 ms on 6f8a053c7161 (executor driver) (1/1)\n",
      "21/11/07 05:41:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/11/07 05:41:53 INFO DAGScheduler: ResultStage 0 (show at cmd5.sc:24) finished in 0.150 s\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/11/07 05:41:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "21/11/07 05:41:53 INFO DAGScheduler: Job 0 finished: show at cmd5.sc:24, took 0.160893 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/07 05:41:53 INFO SparkUI: Stopped Spark web UI at http://6f8a053c7161:4040\n",
      "21/11/07 05:41:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/11/07 05:41:53 INFO MemoryStore: MemoryStore cleared\n",
      "21/11/07 05:41:53 INFO BlockManager: BlockManager stopped\n",
      "21/11/07 05:41:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/11/07 05:41:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/11/07 05:41:53 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5be4c56c\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [A: decimal(1,0), B: decimal(1,0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark =\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"Hello Spark App\")\n",
    "    .config(\"spark.eventLog.enabled\", false)\n",
    "    .getOrCreate()\n",
    "\n",
    "val df = spark.read.\n",
    "  format(\"net.snowflake.spark.snowflake\").\n",
    "  options(Map(\n",
    "      \"useProxy\"   -> \"false\",\n",
    "      \"sfURL\"      -> s\"https://${System.getenv(\"SNOW_ACCOUNT\")}.snowflakecomputing.com:443\",\n",
    "      \"sfAccount\"  -> System.getenv(\"SNOW_ACCOUNT\"),\n",
    "      \"sfUser\"     -> System.getenv(\"SNOW_USER\"),\n",
    "      \"sfPassword\" -> System.getenv(\"SNOW_PASSWORD\"),\n",
    "      \"sfDatabase\" -> System.getenv(\"SNOW_DATABASE\"),\n",
    "      \"sfSchema\"   -> \"PUBLIC\",\n",
    "      \"sfWarehouse\" -> System.getenv(\"SNOW_WAREHOUSE\"),\n",
    "      \"sfRole\" -> System.getenv(\"SNOW_ROLE\"))).\n",
    "  option(\"query\", \"SELECT 1 as A, 2 as B\").\n",
    "  load()\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
